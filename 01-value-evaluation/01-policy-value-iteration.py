import numpy as np
import matplotlib
matplotlib.use('Agg')  # ðŸ‘ˆ å¿…é¡»åœ¨ import pyplot ä¹‹å‰ï¼
import matplotlib.pyplot as plt

# --- å‚æ•° ---
N = 200  # Grid å¤§å° NÃ—N
gamma = 0.999
theta = 1.0
actions = [0, 1, 2, 3]  # up, right, down, left
action_names = {0: '^', 1: '>', 2: 'v', 3: '<'}
n_states = N * N

def generate_z_maze(N):
    maze = np.ones((N, N), dtype=int)  # 1ä»£è¡¨å¢™ï¼Œ0ä»£è¡¨é€šé“
    
    for r in range(N):
        if r % 2 == 0:
            # å¶æ•°è¡Œå…¨éƒ¨é€šé“
            maze[r, :] = 0
        else:
            # å¥‡æ•°è¡Œå…¨éƒ¨å¢™ï¼Œä½†åœ¨å·¦è¾¹æˆ–å³è¾¹ç•™ä¸€ä¸ªé€šé“å£å½¢æˆè¿žæŽ¥
            if ((r - 1) // 2) % 2 == 0:
                maze[r, -1] = 0  # å³ä¾§å¼€å£
            else:
                maze[r, 0] = 0   # å·¦ä¾§å¼€å£
    
    # èµ·ç‚¹å’Œç»ˆç‚¹å¿…é¡»æ˜¯é€šé“
    maze[0, 0] = 0
    maze[N-1, N-1] = 0
    return maze

# æµ‹è¯•æ‰“å°
def print_maze(maze):
    N = maze.shape[0]
    for r in range(N):
        row_str = ""
        for c in range(N):
            row_str += " . " if maze[r, c] == 0 else " # "
        print(row_str)

maze = generate_z_maze(N)
# print("=== è¿·å®«ç»“æž„ ===")
# print_maze(maze)

# --- çŠ¶æ€è½¬ç§» P ---
P = {}
for s in range(n_states):
    P[s] = {}
    row, col = divmod(s, N)
    for a in actions:
        if s in [n_states - 1]:  # ç»ˆç‚¹ï¼ˆå¸æ”¶çŠ¶æ€ï¼‰
            P[s][a] = [(1.0, s, 100)]
        else:
            next_r, next_c = row, col
            if a == 0:  # up
                next_r = max(0, row - 1)
            elif a == 1:  # right
                next_c = min(N - 1, col + 1)
            elif a == 2:  # down
                next_r = min(N - 1, row + 1)
            elif a == 3:  # left
                next_c = max(0, col - 1)

            # æ’žå¢™æ£€æµ‹
            if maze[next_r, next_c] == 1:
                next_r, next_c = row, col

            next_s = next_r * N + next_c
            P[s][a] = [(1.0, next_s, -1)]

# --- Value Iteration ---
# --- æµ‹è¯•ï¼šä»Žèµ·ç‚¹ S å‡ºå‘ï¼Œä½¿ç”¨æœ€ä¼˜ç­–ç•¥æ˜¯å¦èƒ½åˆ°è¾¾ç»ˆç‚¹ T ---
def test_policy_rollout(policy, maze, N, max_steps=None):
    if max_steps is None:
        max_steps = N * N  # æœ€å¤§æ­¥æ•°é™åˆ¶

    action_names = {0: 'up', 1: 'right', 2: 'down', 3: 'left'}
    
    # èµ·ç‚¹
    r, c = 0, 0
    state = r * N + c
    goal = N * N - 1  # ç»ˆç‚¹çŠ¶æ€ç´¢å¼•
    path = [(r, c)]
    

    for step in range(1, max_steps + 1):
        if state == goal:
            print(f"âœ… æˆåŠŸï¼åœ¨ç¬¬ {step-1} æ­¥åˆ°è¾¾ç»ˆç‚¹ T({N-1},{N-1})")
            print(f"ðŸ† è·¯å¾„é•¿åº¦: {len(path)} æ­¥")
            return True

        # èŽ·å–å½“å‰çŠ¶æ€ä¸‹çš„æœ€ä¼˜åŠ¨ä½œ
        action_probs = policy[state]
        action = np.argmax(action_probs)  # ç¡®å®šæ€§ç­–ç•¥


        # æ‰§è¡ŒåŠ¨ä½œ
        next_r, next_c = r, c
        if action == 0:   # up
            next_r = max(0, r - 1)
        elif action == 1: # right
            next_c = min(N - 1, c + 1)
        elif action == 2: # down
            next_r = min(N - 1, r + 1)
        elif action == 3: # left
            next_c = max(0, c - 1)

        # æ£€æŸ¥æ˜¯å¦æ’žå¢™
        if maze[next_r, next_c] != 1:
            r, c = next_r, next_c
            state = r * N + c
            path.append((r, c))

    # è¶…å‡ºæœ€å¤§æ­¥æ•°ä»æœªåˆ°è¾¾
    if state != goal:
        return False

import time

def update_value_iteration(n_states, V, actions, gamma):
    start_time = time.time()
    delta = 0
    for s in range(n_states):
        v = V[s]
        action_values = [sum(p * (r + gamma * V[s2]) for p, s2, r in P[s][a]) for a in actions]
        V[s] = max(action_values)
        delta = max(delta, abs(v - V[s]))
    elapsed = time.time() - start_time
    return delta, elapsed

def value_iteration():
    global theta
    V = np.zeros(n_states)
    deltas = []
    iteration = 0
    totalT = 0
    while True:
        while True:
            delta, elapsed = update_value_iteration(n_states, V, actions, gamma)
            totalT += elapsed
            deltas.append(delta)
            if iteration % 10== 0: 
                print(f"Iteration: {len(deltas)}, Max Delta: {delta:.4f}")
            iteration += 1
            if delta < theta:
                break

        policy = np.zeros((n_states, len(actions)))
        for s in range(n_states):
            action_values = [sum(p * (r + gamma * V[s2]) for p, s2, r in P[s][a]) for a in actions]
            best_action = np.argmax(action_values)
            policy[s] = np.eye(len(actions))[best_action]
            
        if test_policy_rollout(policy, maze, N):
            print(f"âœ… Value Iteration å®Œæˆï¼æ€»è¿­ä»£æ¬¡æ•°: {len(deltas)}, æ€»è€—æ—¶: {totalT:.2f} ç§’")
            return policy, V, deltas
        else:
            print("theta too large, retrying...", theta)
            theta /= 2.0
policy, V, deltas = value_iteration()
print("end = value iteration")

# --- ç»˜åˆ¶ Value Iteration æ”¶æ•›æ›²çº¿å¹¶ä¿å­˜ ---
plt.figure(figsize=(10, 6))
plt.plot(deltas, marker='o', linestyle='-', color='b', markersize=4, linewidth=1.5)
plt.xlabel('Iteration', fontsize=12)
plt.ylabel('Max Delta (Î”)', fontsize=12)
plt.title('Value Iteration Convergence Curve', fontsize=14)
plt.grid(True, alpha=0.3)
plt.yscale('log')  # å¯é€‰ï¼šä½¿ç”¨å¯¹æ•°åæ ‡è§‚å¯Ÿæ”¶æ•›é€Ÿåº¦
plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€

# ä¿å­˜å›¾åƒ
plt.savefig('value_iteration_convergence.png', dpi=200, bbox_inches='tight')
plt.close()  # å…³é—­å›¾å½¢ï¼Œé‡Šæ”¾å†…å­˜

print("âœ… æ”¶æ•›æ›²çº¿å·²ä¿å­˜ä¸º: value_iteration_convergence.png")

# è°ƒç”¨æ‰“å°
print("\n=== è¿·å®«ç»“æž„å’Œæœ€ä¼˜ç­–ç•¥ ===")
# å‡è®¾N, policy, mazeå·²ç»å®šä¹‰å¥½
def plot_maze_with_policy(policy, maze, N, save_path='maze_with_policy.png'):
    # å®šä¹‰åŠ¨ä½œç¬¦å·
    action_symbols = {0: '^', 1: '>', 2: 'v', 3: '<'}
    
    fig, ax = plt.subplots(figsize=(N, N))
    ax.set_aspect('equal')

    # ç»˜åˆ¶ç½‘æ ¼èƒŒæ™¯
    for r in range(N):
        for c in range(N):
            if maze[r, c] == 1:
                facecolor = 'gray'
                text = " "
            elif r * N + c == 0:
                facecolor = 'green'
                text = "S"
            elif r * N + c == n_states - 1:
                facecolor = 'red'
                text = "T"
            else:
                facecolor = 'white'
                a = np.argmax(policy[r * N + c])
                text = action_symbols[a]
            
            rect = plt.Rectangle((c, N-r-1), 1, 1, facecolor=facecolor, edgecolor='black')
            ax.add_patch(rect)
            ax.text(c+0.5, N-r-1+0.5, text, ha='center', va='center', fontsize=16)

    plt.xlim(0, N)
    plt.ylim(0, N)
    ax.axis('off')  # å…³é—­åæ ‡è½´
    plt.savefig(save_path, dpi=200, bbox_inches='tight')
    plt.close()
    print(f"âœ… è¿·å®«å’Œç­–ç•¥å›¾å·²ä¿å­˜ä¸º: {save_path}")

# è°ƒç”¨å‡½æ•°
# plot_maze_with_policy(policy, maze, N)  # è¯·ç¡®ä¿è¿™é‡Œçš„N, policy, mazeæ˜¯ä¹‹å‰å®šä¹‰å¥½çš„å˜é‡