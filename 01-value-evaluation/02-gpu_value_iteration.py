import torch
import time

def prepare_transition_model(P, n_states, actions, device='cuda'):
    """
    å°† P è½¬æ¢ä¸ºé€‚åˆ GPU å¹¶è¡Œçš„å¼ é‡å½¢å¼ï¼š
    next_states[s, a] = ä¸‹ä¸€ä¸ªçŠ¶æ€ç¼–å·
    rewards[s, a] = å¥–åŠ±
    """
    next_states = torch.zeros((n_states, len(actions)), dtype=torch.long, device=device)
    rewards = torch.zeros((n_states, len(actions)), dtype=torch.float32, device=device)

    for s in range(n_states):
        for a in actions:
            p, s2, r = P[s][a][0]  # è¿™é‡Œå‡è®¾ç¡®å®šæ€§è½¬ç§» p=1
            next_states[s, a] = s2
            rewards[s, a] = r

    return next_states, rewards


def update_value_iteration_gpu(n_states, V, actions, gamma, next_states, rewards):
    """
    GPU åŠ é€Ÿçš„ Value Iteration æ›´æ–°
    V: torch.tensor [n_states]  (GPU)
    next_states: torch.long [n_states, n_actions]
    rewards: torch.float32 [n_states, n_actions]
    """
    start_time = time.time()

    # èŽ·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ V å€¼
    V_next = V[next_states]  # shape: [n_states, n_actions]

    # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„ Q å€¼
    action_values = rewards + gamma * V_next

    # æ›´æ–° V
    new_V, _ = torch.max(action_values, dim=1)

    # è®¡ç®— delta
    delta = torch.max(torch.abs(new_V - V)).item()

    V.copy_(new_V)  # åŽŸåœ°æ›´æ–°

    elapsed = time.time() - start_time
    return delta, elapsed


import numpy as np
import matplotlib
matplotlib.use('Agg')  # ðŸ‘ˆ å¿…é¡»åœ¨ import pyplot ä¹‹å‰ï¼
import matplotlib.pyplot as plt

# --- å‚æ•° ---
N = 100  # Grid å¤§å° NÃ—N
gamma = 0.999
theta = 1.0
actions = [0, 1, 2, 3]  # up, right, down, left
action_names = {0: '^', 1: '>', 2: 'v', 3: '<'}
n_states = N * N

def generate_z_maze(N):
    maze = np.ones((N, N), dtype=int)  # 1ä»£è¡¨å¢™ï¼Œ0ä»£è¡¨é€šé“
    
    for r in range(N):
        if r % 2 == 0:
            # å¶æ•°è¡Œå…¨éƒ¨é€šé“
            maze[r, :] = 0
        else:
            # å¥‡æ•°è¡Œå…¨éƒ¨å¢™ï¼Œä½†åœ¨å·¦è¾¹æˆ–å³è¾¹ç•™ä¸€ä¸ªé€šé“å£å½¢æˆè¿žæŽ¥
            if ((r - 1) // 2) % 2 == 0:
                maze[r, -1] = 0  # å³ä¾§å¼€å£
            else:
                maze[r, 0] = 0   # å·¦ä¾§å¼€å£
    
    # èµ·ç‚¹å’Œç»ˆç‚¹å¿…é¡»æ˜¯é€šé“
    maze[0, 0] = 0
    maze[N-1, N-1] = 0
    return maze
maze = generate_z_maze(N)

# --- çŠ¶æ€è½¬ç§» P ---
P = {}
for s in range(n_states):
    P[s] = {}
    row, col = divmod(s, N)
    for a in actions:
        if s in [n_states - 1]:  # ç»ˆç‚¹ï¼ˆå¸æ”¶çŠ¶æ€ï¼‰
            # probability, next state, reward
            P[s][a] = [(1.0, s, 100)]
        else:
            next_r, next_c = row, col
            if a == 0:  # up
                next_r = max(0, row - 1)
            elif a == 1:  # right
                next_c = min(N - 1, col + 1)
            elif a == 2:  # down
                next_r = min(N - 1, row + 1)
            elif a == 3:  # left
                next_c = max(0, col - 1)

            # æ’žå¢™æ£€æµ‹
            if maze[next_r, next_c] == 1:
                next_r, next_c = row, col

            next_s = next_r * N + next_c
            P[s][a] = [(1.0, next_s, -1)]

            
device = 'cuda'
next_states, rewards = prepare_transition_model(P, n_states, actions, device=device)


import time

# --- Value Iteration ---
# --- æµ‹è¯•ï¼šä»Žèµ·ç‚¹ S å‡ºå‘ï¼Œä½¿ç”¨æœ€ä¼˜ç­–ç•¥æ˜¯å¦èƒ½åˆ°è¾¾ç»ˆç‚¹ T ---
def test_policy_rollout(policy, maze, N, max_steps=None):
    if max_steps is None:
        max_steps = N * N  # æœ€å¤§æ­¥æ•°é™åˆ¶

    action_names = {0: 'up', 1: 'right', 2: 'down', 3: 'left'}
    
    # èµ·ç‚¹
    r, c = 0, 0
    state = r * N + c
    goal = N * N - 1  # ç»ˆç‚¹çŠ¶æ€ç´¢å¼•
    path = [(r, c)]
    

    for step in range(1, max_steps + 1):
        if state == goal:
            print(f"âœ… æˆåŠŸï¼åœ¨ç¬¬ {step-1} æ­¥åˆ°è¾¾ç»ˆç‚¹ T({N-1},{N-1})")
            print(f"ðŸ† è·¯å¾„é•¿åº¦: {len(path)} æ­¥")
            return True

        # èŽ·å–å½“å‰çŠ¶æ€ä¸‹çš„æœ€ä¼˜åŠ¨ä½œ
        action_probs = policy[state]
        action = np.argmax(action_probs)  # ç¡®å®šæ€§ç­–ç•¥


        # æ‰§è¡ŒåŠ¨ä½œ
        next_r, next_c = r, c
        if action == 0:   # up
            next_r = max(0, r - 1)
        elif action == 1: # right
            next_c = min(N - 1, c + 1)
        elif action == 2: # down
            next_r = min(N - 1, r + 1)
        elif action == 3: # left
            next_c = max(0, c - 1)

        # æ£€æŸ¥æ˜¯å¦æ’žå¢™
        if maze[next_r, next_c] != 1:
            r, c = next_r, next_c
            state = r * N + c
            path.append((r, c))

    # è¶…å‡ºæœ€å¤§æ­¥æ•°ä»æœªåˆ°è¾¾
    if state != goal:
        return False

def update_value_iteration(n_states, V, actions, gamma):
    start_time = time.time()
    delta = 0
    for s in range(n_states):
        v = V[s]
        action_values = [sum(p * (r + gamma * V[s2]) for p, s2, r in P[s][a]) for a in actions]
        V[s] = max(action_values)
        delta = max(delta, abs(v - V[s]))
    elapsed = time.time() - start_time
    return delta, elapsed

def value_iteration():
    global theta
    V = torch.zeros(n_states, dtype=torch.float32, device=device)
    deltas = []
    iteration = 0
    totalT = 0

    while True:
        while True:
            delta, elapsed = update_value_iteration_gpu(
                n_states, V, actions, gamma, next_states, rewards
            )
            totalT += elapsed
            deltas.append(delta)

            if iteration % 10 == 0:
                print(f"Iteration: {len(deltas)}, Max Delta: {delta:.4f}")

            iteration += 1
            if delta < theta:
                break

        # åœ¨è®¡ç®— policy å‰ï¼ŒæŠŠ V è½¬å›ž CPU numpy
        V_cpu = V.cpu().numpy()

        policy = np.zeros((n_states, len(actions)))
        for s in range(n_states):
            action_values = [
                sum(p * (r + gamma * V_cpu[s2]) for p, s2, r in P[s][a])
                for a in actions
            ]
            best_action = np.argmax(action_values)
            policy[s] = np.eye(len(actions))[best_action]

        if test_policy_rollout(policy, maze, N):
            print(f"âœ… Value Iteration å®Œæˆï¼æ€»è¿­ä»£æ¬¡æ•°: {len(deltas)}, æ€»è€—æ—¶: {totalT:.2f} ç§’")
            return policy, V_cpu, deltas
        else:
            print("theta too large, retrying...", theta)
            theta /= 2.0

policy, V, deltas = value_iteration()
print("end = value iteration")

# --- ç»˜åˆ¶ Value Iteration æ”¶æ•›æ›²çº¿å¹¶ä¿å­˜ ---
plt.figure(figsize=(10, 6))
plt.plot(deltas, marker='o', linestyle='-', color='b', markersize=4, linewidth=1.5)
plt.xlabel('Iteration', fontsize=12)
plt.ylabel('Max Delta (Î”)', fontsize=12)
plt.title('Value Iteration Convergence Curve', fontsize=14)
plt.grid(True, alpha=0.3)
plt.yscale('log')  # å¯é€‰ï¼šä½¿ç”¨å¯¹æ•°åæ ‡è§‚å¯Ÿæ”¶æ•›é€Ÿåº¦
plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€

# ä¿å­˜å›¾åƒ
plt.savefig('value_iteration_convergence.png', dpi=200, bbox_inches='tight')
plt.close()  # å…³é—­å›¾å½¢ï¼Œé‡Šæ”¾å†…å­˜

print("âœ… æ”¶æ•›æ›²çº¿å·²ä¿å­˜ä¸º: value_iteration_convergence.png")

# è°ƒç”¨æ‰“å°
print("\n=== è¿·å®«ç»“æž„å’Œæœ€ä¼˜ç­–ç•¥ ===")
# å‡è®¾N, policy, mazeå·²ç»å®šä¹‰å¥½
def plot_maze_with_policy(policy, maze, N, save_path='maze_with_policy.png'):
    # å®šä¹‰åŠ¨ä½œç¬¦å·
    action_symbols = {0: '^', 1: '>', 2: 'v', 3: '<'}
    
    fig, ax = plt.subplots(figsize=(N, N))
    ax.set_aspect('equal')

    # ç»˜åˆ¶ç½‘æ ¼èƒŒæ™¯
    for r in range(N):
        for c in range(N):
            if maze[r, c] == 1:
                facecolor = 'gray'
                text = " "
            elif r * N + c == 0:
                facecolor = 'green'
                text = "S"
            elif r * N + c == n_states - 1:
                facecolor = 'red'
                text = "T"
            else:
                facecolor = 'white'
                a = np.argmax(policy[r * N + c])
                text = action_symbols[a]
            
            rect = plt.Rectangle((c, N-r-1), 1, 1, facecolor=facecolor, edgecolor='black')
            ax.add_patch(rect)
            ax.text(c+0.5, N-r-1+0.5, text, ha='center', va='center', fontsize=16)

    plt.xlim(0, N)
    plt.ylim(0, N)
    ax.axis('off')  # å…³é—­åæ ‡è½´
    plt.savefig(save_path, dpi=200, bbox_inches='tight')
    plt.close()
    print(f"âœ… è¿·å®«å’Œç­–ç•¥å›¾å·²ä¿å­˜ä¸º: {save_path}")

# è°ƒç”¨å‡½æ•°
plot_maze_with_policy(policy, maze, N)  # è¯·ç¡®ä¿è¿™é‡Œçš„N, policy, mazeæ˜¯ä¹‹å‰å®šä¹‰å¥½çš„å˜é‡